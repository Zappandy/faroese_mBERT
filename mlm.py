# -*- coding: utf-8 -*-
"""MLM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dWb1jSogF1ktiN86X0iWkIo8y7pa9o0o
"""

#!pip install transformers tokenizers datasets


lang = 'maltese'
lang = 'arabic'
TRAIN_BATCH_SIZE = 16
VALID_BATCH_SIZE = 16
SAVE_STEPS = 100
EVAL_STEPS = 50
SAVE_LIMIT = 2
WARMUP_STEPS = 100
EPOCHS = 5
LEARNING_RATE = 1e-4  # 1e-04

#load_dataset("MLRS/korpus_malti")
#https://huggingface.co/MLRS/mBERTu
# https://huggingface.co/docs/datasets/v1.11.0/loading_datasets.html
from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset, Dataset, DatasetDict
import pandas as pd
from dataclasses import dataclass
import torch
import numpy as np
from sklearn.model_selection import train_test_split
#dataset = load_dataset("MLRS/korpus_malti", split=['train[:10%]', 'test[:10%]'])  # this is too slow.
#dataset = load_dataset("MLRS/korpus_malti")
model = AutoModelForMaskedLM.from_pretrained("MLRS/mBERTu")
tokenizer = AutoTokenizer.from_pretrained("MLRS/mBERTu")

# dataset
# train = dataset['train'].shard(num_shards=246, index=0)  # 69k - 70k
# validation = dataset['validation'].shard(num_shards=246, index=0)  # 12257



@dataclass
# 1. We take in a sentence and its tags
# 2. We tokenize the sentence using the tokenizer
# 3. We create a list of tags for each word in the sentence
# 4. We create a list of tags for each token in the sentence
# 5. We create a list of tags for each subtoken in the sentence
# 6. We return a dictionary of the tokenized sentence, the list of tags for each word, and the list of
# tags for each subtoken
class PreDataCollator:
    
    def __init__(self, tokenizer, max_len):

        self.tokenizer = tokenizer
        self.max_len = max_len        
        
    
    def __call__(self, batch):
        
        input_ids = []
        attention_mask = []
        labels = []
        
        for sent in batch['sents']:  # was sentences before
            
            tokenized = self.tokenize(sent)
            input_ids.append(tokenized['input_ids'])
            attention_mask.append(tokenized['attention_mask'])            
            
        
        
        batch = {'input_ids':input_ids,'attention_mask':attention_mask}
        

        return batch

    def tokenize(self, sentence):
        
  

        # using tokenizer to encode sentence (includes padding/truncation up to max length)
        # BertTokenizerFast provides "return_offsets_mapping" functionality for individual tokens, so we know the start and end of a token divided into subtokens
        encoding = self.tokenizer(sentence,                             
                             #return_offsets_mapping=True, 
                             padding='max_length', 
                             truncation=True, 
                             max_length=self.max_len)
 
            

        # turning everything into PyTorch tensors
        item = {key: torch.as_tensor(val) for key, val in encoding.items()}        

        return item



# Commented out IPython magic to ensure Python compatibility.
# %cd 'gdrive/MyDrive/NLP_Projects/lexical_resources'

"""# Generated augmented tokens

New tokens

# Main Paths
"""


corpus = base_path + f'/Languages/{lang}/Corpora/{lang}_corpus.txt'


"""# Load tokenizer

Created after vocabulary augmentation
"""

#corpus
with open(corpus, 'r') as f:
  stored = f.readlines()

train, dev = train_test_split(stored, test_size=0.2)

train_dataset = Dataset.from_dict({"sents": train})
dev_dataset = Dataset.from_dict({"sents": dev})

whole_data = DatasetDict({'train': train_dataset, 'val': dev_dataset})

# dataset = LineByLineTextDataset(
#     tokenizer = tokenizer_path,
#     file_path = corpus,
#     block_size = 128
# )

MAX_LEN = 256
collator = PreDataCollator(tokenizer=tokenizer, max_len=MAX_LEN)
train_tokenized = train_dataset.map(collator, remove_columns=train_dataset.column_names, batch_size=4, num_proc=4, batched=True)
dev_tokenized = dev_dataset.map(collator, remove_columns=dev_dataset.column_names, batch_size=4, num_proc=4, batched=True)

"""# MLM"""

percentage_mask = 0.15
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=percentage_mask)

"""# Pretraining"""

output_dir = f"./Languages/{lang}/pre_trained/"

training_args = TrainingArguments(
    output_dir= output_dir,
    group_by_length=True,
    overwrite_output_dir = True, #replaces the old models everytime we run train it -- so good!
    num_train_epochs=EPOCHS, 
    per_device_train_batch_size=TRAIN_BATCH_SIZE,
    learning_rate= LEARNING_RATE,
    weight_decay= 0.01,
    gradient_accumulation_steps=2,
    evaluation_strategy="steps",
    fp16=False,
    save_steps=SAVE_STEPS,
    eval_steps=EVAL_STEPS,
    logging_steps=EVAL_STEPS,    
    warmup_steps=WARMUP_STEPS,
    save_total_limit=SAVE_LIMIT    
)

# evaluation_strategy

# Perfrom pre-training and save the model
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_tokenized,
    eval_dataset=dev_tokenized,
    tokenizer=tokenizer
)  # may need to pass new tokenizer.


trainer.train()
trainer.save_model(output_dir)
